#5.31.17

#Iris data set example

from sklearn.datasets import load_iris

iris = load_iris()

X = iris.data
y = iris.target

X_new = [[3,5,4,2],[5,4,3,2]]
'''
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X, y)
knn.predict([3,5,4,2])

print(iris)
'''
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X, y)

print(logreg.predict(X_new))


#pandas and knn for machine learning - OUR DATA
for x in possession:
    posinfo.append([x[0]]) #creates a list for each possession

for a in posinfo:
    spead = [] #intermediate speed variable
    disst = [] #intermediate distance variable
    for x,y,d in zip(posA,speedA,passdistA):
        if a[0] == x: #if the possession the speed/dist is from is the same as the info list...
            spead.append(y) #add to intermediate variable
            disst.append(d)
    if len(spead) > 0: #if the speed/dist exsits...
        a.append(sum(spead)/len(spead)) #append average speed to info
        a.append(sum(disst)/len(disst)) #append average distance to info
    else: #if empty, append 0; makes it so there is no division by zero
        a.append(0) 
        a.append(0)
    for v in passperpos: #appends number of passes to posinfo if possession number matches
        if a[0] == v[2]:
            a.append(v[1])
        
posinfo1 = [] #[speed, distance, #passes]
for x in posinfo:
    if x[3] != 0 and x[1] != 0:
        posinfo1.append(x[1:])
    elif x[1] == x[2] == x[3] == 0:
        posinfo1.append(x[1:])
    else:
        notincluded.append(x[0])

outcome = [1,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,0,1,1,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0] #1=made shot; 0=any other response
#outcome = [1,1,1,1,1,1,0,0,1,1,1,1,1,0,1]

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

X = posinfo1
y = outcome

logreg = LogisticRegression()

logreg.fit(X, y)

#X_new = [20,10,4]
y_pred = logreg.predict(X)

print(logreg.predict(X))

print(metrics.accuracy_score(y, y_pred))

'''
from sklearn.neighbors import KNeighborsClassifier

X = posinfo1
y = outcome

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X,y)
#print(knn.predict([6,10,21]))
'''
newpos= [6,10,21]

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X,y)
response = knn.predict(newpos)
'''
from sklearn import metrics
from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=(25/34))

knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(X_train,y_train)
knn.predict(X_test)
kpred = knn.predict(X_test)
print(metrics.accuracy_score(y_test,kpred))
'''
if response == 1:
    print('Shot attempted.')
else:
    print('No shot.')
    
    
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

#y_pred = logreg.predict(X_test)
#print(metrics.accuracy_score(y_test, y_pred))
'''
k_range = range(1, 21)
scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    scores.append(metrics.accuracy_score(y_test, y_pred))
   
import matplotlib.pyplot as plt

# allow plots to appear within the notebook
#%matplotlib inline

# plot the relationship between K and testing accuracy
plt.plot(k_range, scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Testing Accuracy')


#Possible code to use for a 3d graph
#ignore lines 144-194
print(__doc__)


# Code source: GaÃ«l Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
from sklearn.decomposition import PCA

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

plt.figure(2, figsize=(8, 6))
plt.clf()

# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())

# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
X_reduced = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,
           cmap=plt.cm.Paired)
ax.set_title("First three PCA directions")
ax.set_xlabel("1st eigenvector")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel("2nd eigenvector")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel("3rd eigenvector")
ax.w_zaxis.set_ticklabels([])

plt.show()
